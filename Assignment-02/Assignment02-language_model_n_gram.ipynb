{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:How to Github and Why do we use Jupyter and Pycharm?<br />\n",
    "A1：Jupyter易于交互，Pycharm易于调试\n",
    "\n",
    "Q2:What's the Probability Model?<br />\n",
    "A2:概率模型输出事件发生的概率。从数学的角度来解释，概率模型给出了一个函数$f(x_1,x_2,x_3...x_n)$，当给定随机变量$x_1,x_2,x_3...x_n$时，模型将输出一个概率$P$，且有$0\\leqslant P\\leqslant 1$.\n",
    "\n",
    "Q3:Can you came up with some sceneraies at which we could use Probability Model?<br />\n",
    "A3:分类问题都用到了概率模型.\n",
    "\n",
    "Q4:Why do we use probability and what's the difficult points for programming based on parsing and pattern match?<br />\n",
    "A4:用概率表示能有一个直观的可能性印象，而且便于结果之间的比较；模式匹配的难点在于规则有限，问句变化不够灵活.\n",
    "\n",
    "Q5:What's the Language Model?<br />\n",
    "A5:语言模型，输入一句话，输出这句话合理的概率.\n",
    "\n",
    "Q6:Can you came up with some sceneraies at which we could use Language Model?<br />\n",
    "A6:对文本进行语义分析；语音识别；机器翻译.\n",
    "\n",
    "Q7:What's the 1-gram language model?<br />\n",
    "A7:以一个词为单位，计算这个词出现的可能性. 假设用1-gram模型计算句子S出现的合理性，经过分词后$S=(w_1,w_2,w_3,...,w_n)$，则$$P(S)=\\prod_{i=1}^nP(w_i)$$.\n",
    "\n",
    "Q8:What's the disadvantages and advantages of 1-gram language model?<br />\n",
    "A8:缺点：模型效果依赖于分词效果;忽略了当前词与其相邻词之间的关联关系，词出现的概率仅和当前词在语料库中出现的概率有关.<br />\n",
    "    优点：模型简单，易于计算.\n",
    "\n",
    "Q9:What't the 2-gram model?<br />\n",
    "A9:计算当前词出现的概率时，假设仅与前一个词相关.假设用2-gram模型计算句子S出现的合理性，经过分词后$S=(w_1,w_2,w_3,...,w_n)$，则$$P(S)=\\prod_{i=2}^nP(w_i|w_{i-1})$$.\n",
    "\n",
    "Q10:what's the web crawler, and can you implement a simple crawler?<br />\n",
    "A10:web crawler，从网页的源码中抓取所需要的信息\n",
    "\n",
    "Q11:There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?<br />\n",
    "A11:访问网页时，需要输入验证码，一般来说可以对验证码进行图像识别<br />\n",
    "    需解析网页的地址发生变化<br />\n",
    "    IP限制，可以设置一个IP代理池，每次访问时随机设置代理\n",
    "    \n",
    "Q12:What't the Regular Expression and how to use?<br />\n",
    "A12:正则表达式是通过自己构建模式，去寻找匹配这种模式的字符串.在python中利用re包使用正则表达式.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现n-gram的语言模型<br />\n",
    "使用语料：zhwiki-20190401-pages-articles.xml.bz，以10M大小对语料进行分割，本程序使用其中10个语料文件<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    regex = re.compile('<doc(.*)>')\n",
    "    regex2 = re.compile('（）')\n",
    "    regex3 = re.compile('《》')\n",
    "    regex4 = re.compile('「')\n",
    "    regex5 = re.compile('」')\n",
    "    regex6 = re.compile('</doc>')\n",
    "    string = regex.sub('',string)\n",
    "    string = regex2.sub('',string)\n",
    "    string = regex3.sub('',string)\n",
    "    string = regex4.sub('',string)\n",
    "    string = regex5.sub('',string)\n",
    "    string = regex6.sub('',string)\n",
    "    return ' '.join(re.findall('[\\w\\d]+',string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Covert_to_Simplified(string):\n",
    "    return HanziConv.toSimplified(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): \n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(word,frequence_sum,words_count):\n",
    "    esp = 1/frequence_sum\n",
    "    if word in words_count:\n",
    "        return words_count[word]/frequence_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_prob(words,_n_gram_sum,_n_gram_counter):\n",
    "    words = ''.join(words)\n",
    "    esp = 1/_n_gram_sum\n",
    "    if words in _n_gram_counter:\n",
    "        return _n_gram_counter[words]/_n_gram_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_n_gram(sentence,n,frequence_sum,words_count,_n_gram_sum,_n_gram_counter):\n",
    "    probability = 1\n",
    "    words = cut(sentence)\n",
    "    for i,word in enumerate(words):\n",
    "        if i<n:\n",
    "            prob = get_prob(word,frequence_sum,words_count)\n",
    "        else:\n",
    "            previous = words[i-n:i]\n",
    "            prob = get_n_gram_prob(previous+list(word),_n_gram_sum,_n_gram_counter)\n",
    "        probability *= prob\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理wiki中文语料\n",
    "def get_articles(size):\n",
    "    all_articles = []\n",
    "    for i in range(size):\n",
    "        if i<10:\n",
    "            file = 'extracted/AA/wiki_' + '0' + str(i)\n",
    "        else:\n",
    "            file = 'extracted/AA/wiki_' + str(i)\n",
    "        with open(file,'r',encoding='utf8') as fl:\n",
    "            all_articles += fl.readlines()\n",
    "    all_articles = [token(str(s)) for s in all_articles if s.strip()!='']\n",
    "    all_articles = [Covert_to_Simplified(s) for s in all_articles]\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(size):\n",
    "    #使用语料文件数量\n",
    "    all_articles = get_articles(size)\n",
    "    text = ''\n",
    "    for sentence in all_articles:\n",
    "        text += sentence\n",
    "    TEXT = text\n",
    "    \n",
    "    #对所有语料进行分词\n",
    "    all_tokens = cut(TEXT)\n",
    "    #得到有效的分词结果\n",
    "    valid_tokens = [t for t in all_tokens if t.strip() and t!='n']\n",
    "    \n",
    "    #计数\n",
    "    words_count = Counter(valid_tokens)\n",
    "    frequence = [f for w,f in words_count.most_common()]\n",
    "    frequence_sum = sum(frequence)\n",
    "    \n",
    "    return frequence_sum,words_count,valid_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_data(n,valid_tokens):\n",
    "    #决定n-gram数目，并将分词结果以n进行组合\n",
    "    all_n_gram_words = [''.join(valid_tokens[i:i+n]) for i in range(len(valid_tokens[:-n]))]\n",
    "    _n_gram_sum = len(all_n_gram_words)\n",
    "    _n_gram_counter = Counter(all_n_gram_words)\n",
    "    \n",
    "    return _n_gram_sum, _n_gram_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上没有月亮 is more possible\n",
      "----今天晚上没有太阳 with probility 1.700331517555341e-23\n",
      "----今天晚上没有月亮 with probility 1.700331517555341e-23\n",
      "你有什么意见 is more possible\n",
      "----你有什么一件 with probility 2.785473316121813e-21\n",
      "----你有什么意见 with probility 2.785473316121813e-21\n",
      "今天我去吃好吃的 is more possible\n",
      "----我去吃好吃的，今天 with probility 1.0096143975983879e-43\n",
      "----今天我去吃好吃的 with probility 6.095342142292449e-37\n",
      "数学是一门学问 is more possible\n",
      "----数学是一门学科 with probility 7.237734644855822e-21\n",
      "----数学是一门学问 with probility 7.237734644855822e-21\n",
      "点餐的是小明 is more possible\n",
      "----点餐的是一只猫 with probility 4.126069132229608e-30\n",
      "----点餐的是小明 with probility 7.038639464504087e-23\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    size = 10\n",
    "    n = 2\n",
    "    \n",
    "    frequence_sum,words_count,valid_tokens = get_data(size)\n",
    "    _n_gram_sum, _n_gram_counter = get_n_gram_data(n,valid_tokens)\n",
    "    \n",
    "    need_compared = [\n",
    "    \"今天晚上没有太阳 今天晚上没有月亮\",\n",
    "    \"你有什么一件 你有什么意见\",\n",
    "    \"我去吃好吃的，今天 今天我去吃好吃的\",\n",
    "    \"数学是一门学科 数学是一门学问\",\n",
    "    \"点餐的是一只猫 点餐的是小明\"\n",
    "    ]\n",
    "\n",
    "    for s in need_compared:\n",
    "        s1,s2 = s.split()\n",
    "        p1,p2 = language_model_n_gram(s1,n,frequence_sum,words_count,_n_gram_sum,_n_gram_counter), language_model_n_gram(s2,n,frequence_sum,words_count,_n_gram_sum,_n_gram_counter)\n",
    "\n",
    "        better = s1 if p1>p2 else s2\n",
    "\n",
    "        print('{} is more possible'.format(better))\n",
    "        print('-'*4+'{} with probility {}'.format(s1,p1))\n",
    "        print('-'*4+'{} with probility {}'.format(s2,p2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?<br />\n",
    "A:优点：在语料库足够大的情况下，几乎能对所有句子进行合理性判断<br />\n",
    "缺点：模型效果依赖于分词，在分词结果不够准确时，也会对结果造成一定的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: How did you solve this problem in your programming task?<br />\n",
    "A1：给定一个默认值，当某个词不在语料库中时，默认词频为1\n",
    "\n",
    "Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task<br />\n",
    "A2：Turing-Good estimator可以处理数据稀疏的问题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
